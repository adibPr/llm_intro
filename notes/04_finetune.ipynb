{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6474b0a3-b63f-4a75-8c5e-32def3b53e63",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Any\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from trl import setup_chat_format\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from transformers import TrainingArguments\n",
    "import datasets \n",
    "\n",
    "def dump_dataset():\n",
    "    # Convert dataset to OAI messages\n",
    "    system_message = \"\"\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "    SCHEMA:\n",
    "    {schema}\"\"\"\n",
    "\n",
    "    def create_conversation(sample):\n",
    "      return {\n",
    "        \"messages\": [\n",
    "          {\"role\": \"system\", \"content\": system_message.format(schema=sample[\"context\"])},\n",
    "          {\"role\": \"user\", \"content\": sample[\"question\"]},\n",
    "          {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n",
    "        ]\n",
    "      }\n",
    "\n",
    "    # Load dataset from the hub\n",
    "    dataset = datasets.load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
    "    # dataset = dataset.shuffle().select(range(12500))\n",
    "\n",
    "    # Convert dataset to OAI messages\n",
    "    dataset = dataset.map(create_conversation, remove_columns=dataset.features,batched=False)\n",
    "    # split dataset into 10,000 training samples and 2,500 test samples\n",
    "    dataset = dataset.train_test_split(test_size=2500/12500)\n",
    "\n",
    "    # save datasets to disk\n",
    "    dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\n",
    "    dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")\n",
    "\n",
    "\n",
    "def load_dataset(path: str) -> list[dict[str, Any]]:\n",
    "    with open(path, 'r') as f_:\n",
    "        data = json.load(f_)\n",
    "\n",
    "    final_data = []\n",
    "    for d in data:\n",
    "        final_data.append(create_conversation_as_prompt(d))\n",
    "\n",
    "    return final_data\n",
    "\n",
    "def create_conversation(row) -> str:\n",
    "    system_message = \"\"\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "{schema}\"\"\"\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_message.format(schema=row['context'])},\n",
    "            {\"role\": \"user\", \"content\": row['question']},\n",
    "            {\"role\": \"assistant\", \"content\": row['answer']}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def create_conversation_as_prompt(row) -> dict[str, str]:\n",
    "    query_prompt = f\"\"\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "{row['context']}\n",
    "User:\n",
    "{row['question']}\"\"\"\n",
    "    query_answer = f\"\"\"Assistant: \n",
    "{row['answer']}\"\"\"\n",
    "    return {\"prompt\": query_prompt, \"response\": query_answer, 'text': query_prompt + '\\n' + query_answer}\n",
    "\n",
    "\n",
    "PATH_DATA = '../data/sql-create-context/sql_create_context_v4.json'\n",
    "MODEL_ID = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
    "\n",
    "dump_dataset()\n",
    "# data = load_dataset(PATH_DATA)\n",
    "# take 80% as training\n",
    "# data_train = data[:int(0.8 * len(data))][:50]\n",
    "# data_test = data[len(data_train):]\n",
    "\n",
    "data_train = datasets.load_dataset('json', data_files=\"train_dataset.json\", split='train')\n",
    "\n",
    "# dump to json, since Dataset requires JSON\n",
    "# with open('../data/sql-create-context/data_train.json', 'w') as f_:\n",
    "#     for d in data_train:\n",
    "#         f_.write(json.dumps(d) + '\\n')\n",
    "# dataset_train = datasets.load_dataset('json', data_files='../data/sql-create-context/data_train.json')\n",
    "# dataset_train = pd.DataFrame(data_train)\n",
    "dataset_train = data_train.select(range(50))\n",
    "print(len(dataset_train))\n",
    "# for d in dataset_train:\n",
    "#     print(d)\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\", # since we support flash attention `torch.cuda.get_device_capability()[0] >= 8`\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.padding_side = 'right' # to prevent warnings\n",
    "\n",
    "## since TinyLlama is adhere to OAI chatML,  we do not need to setup it. \n",
    "## this is used to add new token to the tokenizer as part of conversation\n",
    "# model, tokenizer = setup_chat_format(model, tokenizer) \n",
    "\n",
    "# Using qlora\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.05,\n",
    "        r=256,\n",
    "        bias=\"none\",\n",
    "        target_modules=\"all-linear\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"code-llama-7b-text-to-sql\", # directory to save and repository id\n",
    "    num_train_epochs=10,                     # number of training epochs\n",
    "    per_device_train_batch_size=1,          # batch size per device during training\n",
    "    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    logging_steps=10,                       # log every 10 steps\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
    "    bf16=True,                              # use bfloat16 precision\n",
    "    tf32=True,                              # use tf32 precision\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
    "    push_to_hub=False,                       # push model to hub\n",
    "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    ")\n",
    "\n",
    "max_seq_length = 3072 # max sequence length for model and packing of the dataset\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset_train,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    # dataset_text_field='text',\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # We template with special tokens\n",
    "        \"append_concat_token\": False, # No need to add additional separator token\n",
    "    }\n",
    ") \n",
    "\n",
    "# start training, the model will be automatically saved to the hub and the output directory\n",
    "trainer.train()\n",
    "\n",
    "# save model\n",
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdf63961-c908-4d48-9753-128a0e286738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/D/miniconda/envs/llm_intro/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_huggingface'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextLoader\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatHuggingFace\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline, AutoTokenizer, AutoModelForCausalLM\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_huggingface'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import uvicorn\n",
    "import json\n",
    "from typing import Any\n",
    "\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from logging.handlers import TimedRotatingFileHandler\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import langchain\n",
    "langchain.verbose = True\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import ChatHuggingFace\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "experiment = 'base'\n",
    "# experiment = 'finetuned'\n",
    "if experiment == 'base': \n",
    "    MODEL_ID = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        trust_remote_code=True,\n",
    "        device_map='auto',\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "elif experiment == 'finetuned': \n",
    "    print(\"Finetuned!\")\n",
    "    model_path = './code-llama-7b-text-to-sql'\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path, \n",
    "        trust_remote_code=True,\n",
    "        device_map='auto',\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "else:\n",
    "    raise ValueError(f\"Unrecognize experiment {experiment}\")\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "generate_text = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    trust_remote_code=True,\n",
    "    max_new_tokens=1200,\n",
    "    repetition_penalty=1.1,\n",
    "    model_kwargs={\n",
    "        \"device_map\": \"auto\",\n",
    "        \"max_length\": 1200,\n",
    "        \"temperature\": 0.0,\n",
    "        \"torch_dtype\":torch.bfloat16}\n",
    "    )\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "llm_chat = ChatHuggingFace(llm=llm)\n",
    "\n",
    "def convert_to_prompt(d: dict[str, Any]):\n",
    "    chat = []\n",
    "    for m in d['messages'][:-1]:\n",
    "        chat.append((m['role'], m['content']))\n",
    "    return ChatPromptTemplate(chat)\n",
    "\n",
    "def print_data(d: dict[str, Any]):\n",
    "    for m in d['messages']:\n",
    "        print('{}: {}'.format(m['role'], m['content']))\n",
    "\n",
    "data =  []\n",
    "with open('train_dataset.json', 'r') as f_:\n",
    "    for l in f_:\n",
    "        data.append(json.loads(l))\n",
    "\n",
    "sample_idx = 15 \n",
    "print(\"Training: \")\n",
    "print_data(data[sample_idx])\n",
    "# print(data[sample_idx]['prompt'])\n",
    "# print(data[sample_idx]['response'])\n",
    "\n",
    "prompt = convert_to_prompt(data[sample_idx])\n",
    "# result = (prompt | llm_chat.bind(skip_prompt=True)).invoke({})\n",
    "result = (prompt | llm_chat).invoke({})\n",
    "print(\"\\n\\n\")\n",
    "print(\"LLM: \")\n",
    "if hasattr(result, 'content'):\n",
    "    print(result.content)\n",
    "else:\n",
    "    print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
