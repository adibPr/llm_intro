{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "102ff4b4-2396-483d-bae5-7af3d833c102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0a419f1-ac64-4747-86d9-8b4016104a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure GOOGLE_API_KEY (or any other llm provider, must exactly the same)\n",
    "_ = load_dotenv(find_dotenv())\n",
    "# or put it into variable and pass it to llm model \n",
    "# google_api_key = os.getenv('GOOGLE_API_KEY') # pass this to the model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c517e1-9d95-4261-9986-ced68de1491a",
   "metadata": {},
   "source": [
    "# Model Initiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34f617e-af7d-47e1-bfb5-1f974a54c3cf",
   "metadata": {},
   "source": [
    "LangChain provides a class to support various LLM models, including OpenAI, Google Gemini, and Meta's LLaMA.\n",
    "Initializing a model is simple—import the correct class and pass the model name and credentials (if not set in the environment).\n",
    "For example, to use Google Gemini, import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b46ed08c-23dc-4c12-b69a-c19c89c414b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/D/miniconda/envs/llm_intro/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2063db13-9087-437f-872b-cfd5bb674373",
   "metadata": {},
   "source": [
    "And then initialize the model with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef1a9760-3451-4d4a-8a5e-a2eba653d87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# available model in: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070ef716-c12a-45df-b0fc-db476f0bac66",
   "metadata": {},
   "source": [
    "# Query the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da3fc87-363d-4d8c-ac41-9644a9357db1",
   "metadata": {},
   "source": [
    "After initializing the model, you can query it using the `llm.invoke()` function with your prompt as a string argument.\n",
    "This function returns a LangChain object with a content attribute that holds the model's response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d95d0e9-443c-4a0d-b558-438ccedfb785",
   "metadata": {},
   "source": [
    "### Simple Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67bc93ff-b3ac-4d98-8b55-4daaefa92990",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(\"What is the good thing we can do to try LLM? Answer in 3 sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a01978d-f17d-4897-9eb8-7492a74ff61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(response) = <class 'langchain_core.messages.ai.AIMessage'>\n",
      "Start with a simple, well-defined prompt to test its core capabilities.  Experiment with different phrasing of the same request to see how it handles variations.  Gradually increase complexity to explore its strengths and limitations in a controlled manner.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"type(response) = {type(response)}\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4282b334-f459-45d2-8ff9-abea1d1a5676",
   "metadata": {},
   "source": [
    "Apart from simple invocation, `llm.invoke()` supports three different input formats:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853f3882-c4ee-4d84-a1bb-19121d3712a1",
   "metadata": {},
   "source": [
    "### Using PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045d98d7-22e4-4ce1-b681-68ad5479a0a3",
   "metadata": {},
   "source": [
    "A template allows you to create dynamic prompts by inserting variables into a predefined structure.\n",
    "In LangChain, we use `PromptTemplate` for this purpose.\n",
    "\n",
    "Similar to the llm object, a prompt template also has an `invoke()` function to process input and generate the final prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adf6803f-717b-4024-8d37-749d193a91b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Automate tasks, generate creative content.\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-23f312bc-c5cc-4b78-9aff-1f6a37f44cc9-0', usage_metadata={'input_tokens': 24, 'output_tokens': 9, 'total_tokens': 33, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "# Using template usually for passing arguments\n",
    "template = \"\"\"\n",
    "What is the good thing we can do with LLM? Write in less than {max_words} words.\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=['max_words'])\n",
    "llm.invoke(prompt.invoke({'max_words': 10}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2290781-d66f-4f35-bac2-a0bd361275e5",
   "metadata": {},
   "source": [
    "LangChain overrides the pipe operator (|) to simplify chaining operations, making it easier to process prompts and model responses sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec72b1f4-484f-4232-8da8-610869ddd545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Innovate\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or using chain\n",
    "chain = prompt | llm # equal to llm.invoke(prompt.invoke({}))\n",
    "chain.invoke({'max_words': 1}).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d470473-98ae-462c-ac19-c157de964fdd",
   "metadata": {},
   "source": [
    "### Using Chat with Roles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf26c8c-da49-443a-a32c-26f902407da5",
   "metadata": {},
   "source": [
    "LLM models also accept input in the form of chat messages.\n",
    "In LangChain, each chat message is categorized into three distinct roles:\n",
    "\n",
    "* System – Provides initial instructions (typically used only in the first message).\n",
    "* Human – Represents user input or prompts.\n",
    "* AI – Represents the LLM's response.\n",
    "\n",
    "These roles align with OpenAI’s categories but use slightly different wording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03f80b85-b96c-4abf-9585-a85579e2dcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try a caprese salad!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chats = (\n",
    "    [\n",
    "        SystemMessage(content=\"\"\"\n",
    "            You are a nice AI bot that helps a user figure out what to eat in one short sentence. \n",
    "            You can only respond to questions regarding food. \n",
    "            Any other questions that are asked not related to food will be answered with \"I don't know\" \n",
    "            \"\"\"),\n",
    "        HumanMessage(content=\"I like tomatoes, what should I eat?\")\n",
    "    ]\n",
    ")\n",
    "response = llm.invoke(chats)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159d4afc-321d-4f5c-b002-8d6c650c902f",
   "metadata": {},
   "source": [
    "Trying unrelated question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4e23f84-1969-45e8-bc1b-7ff99744ac82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n",
      "\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "chats = (\n",
    "    [\n",
    "        SystemMessage(content=\"\"\"\n",
    "            You are a nice AI bot that helps a user figure out what to eat in one short sentence. \n",
    "            You can only respond to questions regarding food. \n",
    "            Any other questions that are asked not related to food will be answered with \"I don't know\" \n",
    "            \"\"\"),\n",
    "        HumanMessage(content=\"Who are the winner of 2024 world cup?\")\n",
    "    ]\n",
    ")\n",
    "response = llm.invoke(chats)\n",
    "print(response.content)\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed00107b-8ace-4c3a-84d6-0bea37bc350a",
   "metadata": {},
   "source": [
    "The return value of invoke() from the LLM is an AIMessage, which matches the chat format we've set up. This makes it easy to add the model's response to the ongoing chat history for future queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a8bc534-da44-4685-a3d7-d2dac5106fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>  Who are the founder of Transformer architecture?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer architecture was primarily created by researchers at Google in 2017.  The key paper, \"Attention is All You Need,\" lists Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin as authors.  While all contributed significantly, it's difficult to pinpoint a single \"founder.\"  The paper represents a collaborative effort.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>  Who have been to the moon?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have the context to answer questions outside of artificial intelligence.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>  break\n"
     ]
    }
   ],
   "source": [
    "history = [\n",
    "    SystemMessage(content=\"\"\"\n",
    "        You are a bot that will help a user to answer their question regarding AI. \n",
    "        Any other questions that are not part of AI, simply answer that you don't have the context for it\n",
    "    \"\"\")\n",
    "]\n",
    "\n",
    "while True:\n",
    "    next_sent = input(\">> \")\n",
    "    if next_sent == \"break\":\n",
    "        break\n",
    "    # convert the input into HummanMessage class, and append it to history\n",
    "    history.append(HumanMessage(content=next_sent))\n",
    "    \n",
    "    # get responose\n",
    "    response = llm.invoke(history)\n",
    "    print(response.content)\n",
    "\n",
    "    # add the response to the history \n",
    "    history.append(response)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "862bb31e-580d-4acb-a8f6-6ddc915d7ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system -- \n",
      "        You are a bot that will help a user to answer their question regarding AI. \n",
      "        Any oth\n",
      "human -- Who are the founder of Transformer architecture?\n",
      "ai -- The Transformer architecture was primarily created by researchers at Google in 2017.  The key paper,\n",
      "human -- Who have been to the moon?\n",
      "ai -- I don't have the context to answer questions outside of artificial intelligence.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check for history\n",
    "for h in history:\n",
    "    print(h.type + \" -- \" + h.content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695b6f03-9260-4c24-8309-568efa1fd3fa",
   "metadata": {},
   "source": [
    "### Chat with Roles and Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c8600e-8aeb-4dc5-b139-73b9abbb5b1d",
   "metadata": {},
   "source": [
    "We can combine prompting with chat roles easily. \n",
    "Instead of manually creating instances of classes like SystemMessage, HumanMessage, and AIMessage, we can use a simpler input format: a pair of (role, content)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8d001d2-10fa-403f-84c6-a60eaff37ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN, atau Convolutional Neural Network,  tidak memiliki penemu tunggal dan tanggal penemuan yang pasti. Konsepnya berkembang secara bertahap, dengan beberapa kontribusi penting di tahun 1980-an.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see: https://python.langchain.com/docs/concepts/prompt_templates/\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# the format is different, with system, user, and assistants category inside a tuple\n",
    "\n",
    "MAX_WORD=50\n",
    "LANGUAGE='Bahasa'\n",
    "\n",
    "history = [\n",
    "    # instead of creating a object, we pass pair (role, content)\n",
    "    (\"system\", \"\"\"\n",
    "        You are a bot that will help a user to answer their question regarding AI. \n",
    "        Any other questions that are not part of AI, simply answer that you don't have the context for it.\n",
    "        You only need to answer at most {max_word} words and use the language of {language}\n",
    "    \"\"\"\n",
    "    ),\n",
    "    (\"user\", \"When the CNN is invented?\")\n",
    "]\n",
    "\n",
    "# build prompt first\n",
    "history_prompt = ChatPromptTemplate(history).invoke({'max_word': MAX_WORD, 'language': LANGUAGE})\n",
    "response = llm.invoke(history_prompt) # or  \n",
    "\n",
    "# or with chain operator\n",
    "# response = (ChatPromptTemplate(history) | llm).invoke({'max_word': MAX_WORD, 'language': LANGUAGE})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72d266c-168a-419b-8cff-c9a1f978dc8e",
   "metadata": {},
   "source": [
    "# Structured Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83770cb-fd0b-4723-a94f-17f179a78d0b",
   "metadata": {},
   "source": [
    "Some llm models support returning in a specific and structured output.\n",
    "This can be achieved by first creating the base class, with it's description and potential value, then call the invoke as usual.\n",
    "What's difference is that the LLM will try to convert the answer into the class that we provided, thus making it easier to be used in the next process.\n",
    "We use `pydantic` to create a class with type annotation and description.\n",
    "\n",
    "See: https://python.langchain.com/docs/how_to/structured_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ed8d55c6-2acb-4efa-a212-9f1db9402cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95ccbec8-2892-4288-9fe9-4d020b149c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Country(BaseModel):\n",
    "    name: str = Field(description='The country name')\n",
    "    capital: str = Field(description='The capital country')\n",
    "    income_per_capita_usd: int = Field(description='The income per capita in USD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f87f3a66-7ffe-4ef3-a256-c204c99a131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_structured = llm.with_structured_output(Country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "39fa2448-880f-4cb3-94db-f35d803110c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country(name='USA', capital='Washington', income_per_capita_usd=65000)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_structured.invoke(\"USA\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
